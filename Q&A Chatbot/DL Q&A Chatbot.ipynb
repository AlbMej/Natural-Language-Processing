{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Question & Answer Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing a chat bot that can answer questions given a set of sentences. The chatbot will use a subset of the Babi Data Set from Facebook Research; it already contains stories(sentences), queries(questions), and answers. \n",
    "Here is a link to the Babi Data Sets and the research paper this is based on:\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "  \n",
    "The bot currently returns a yes or a no to each question asked. However, I plan on integrating a Natural Language Generation component to introduce some meaningful dialogue as well as a speech to text component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating the model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10:1 ratio for training data vs testing data; there are 10,000 points for train_data and 1,000 points for test_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sandra',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'office',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'office', '?'],\n",
       " 'yes')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_sentence = ' '.join(train_data[10][0]) #Story/Sentence\n",
    "query_question = ' '.join(train_data[10][1]) #Query/Question\n",
    "answer = train_data[10][2] #Answer to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Sandra went back to the hallway . Sandra moved to the office .\n",
      "Question:  Is Sandra in the office ?\n",
      "Answer:    yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: \", story_sentence)\n",
    "print(\"Question: \", query_question)\n",
    "print(\"Answer:   \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vocabulary of all of the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set that contains the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + test_data \n",
    "\n",
    "for story, question , answer in all_data:\n",
    "    # Creates a vocabulary of all the distinct words inside our dataset \n",
    "    vocab = vocab | set(story) # vocab ∪ Story. Continuously adds unique words\n",
    "    vocab = vocab | set(question) # vocab ∪ question. Continuously adds unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the two possible answers \n",
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for Keras pad_sequences \n",
    "vocab_size = len(vocab) + 1 # + 1 to add an extra space for a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest story\n",
    "longestStory = max(( (len(data[0])) for data in all_data )) \n",
    "longestQuery = max(( (len(data[1])) for data in all_data )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(longestStory)\n",
    "print(longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates integer encoding for the sequences of words\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(vocab) # This method creates the vocabulary index based on word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hallway': 1,\n",
       " 'down': 2,\n",
       " 'moved': 3,\n",
       " 'journeyed': 4,\n",
       " 'mary': 5,\n",
       " 'discarded': 6,\n",
       " 'in': 7,\n",
       " 'took': 8,\n",
       " 'left': 9,\n",
       " 'got': 10,\n",
       " '?': 11,\n",
       " '.': 12,\n",
       " 'john': 13,\n",
       " 'went': 14,\n",
       " 'dropped': 15,\n",
       " 'put': 16,\n",
       " 'daniel': 17,\n",
       " 'sandra': 18,\n",
       " 'bedroom': 19,\n",
       " 'kitchen': 20,\n",
       " 'apple': 21,\n",
       " 'back': 22,\n",
       " 'bathroom': 23,\n",
       " 'travelled': 24,\n",
       " 'yes': 25,\n",
       " 'the': 26,\n",
       " 'football': 27,\n",
       " 'garden': 28,\n",
       " 'to': 29,\n",
       " 'milk': 30,\n",
       " 'is': 31,\n",
       " 'there': 32,\n",
       " 'no': 33,\n",
       " 'picked': 34,\n",
       " 'office': 35,\n",
       " 'up': 36,\n",
       " 'grabbed': 37}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStoryText = []\n",
    "trainQueryText = []\n",
    "trainAnswers = []\n",
    "\n",
    "for story, query, answer in train_data:\n",
    "    trainStoryText.append(story)\n",
    "    trainQueryText.append(query)\n",
    "    trainAnswers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Transforms each word in the sentences to a sequence of integers.\n",
    "trainStorySeq = tokenizer.texts_to_sequences(trainStoryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeStories(data, word_index = tokenizer.word_index, maxStoryLen = longestStory, maxQueryLen = longestQuery):\n",
    "    \"\"\"\n",
    "    Vectorizes stories, queries, & answers into padded sequences. \n",
    "   \n",
    "    Parameters: \n",
    "        data: All the data (Stories, Queries, Answers)\n",
    "        word_index: A word index dictionary. Defaulted to our tokenizer.word_index\n",
    "                    Can be overrided to other datasets or other sets of questions\n",
    "        maxStoryLen: Length of the longest story (Will be used for the pad_sequences function)\n",
    "        maxQueryLen: Length of the longest query (Will be used for the pad_sequences function)\n",
    "        \n",
    "        We need the max story & query length because we are using padded sequences; not every story/query \n",
    "        is the same length and our RNN that we're using for training needs everything to be the same length \n",
    "        We'll pad the inputs with 0s in case there's a story or query that is too short. Or we can cut down \n",
    "        a story or query if it is too long.\n",
    "        \n",
    "    Returns: \n",
    "        this (tuple): A tuple of the form (X, Q, A) (padded based on max lengths)    \n",
    "    \"\"\"\n",
    "    X = [] # X := Stories\n",
    "    Q = [] # Q := Queries\n",
    "    A = [] # A := Answers (yes/no)\n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        # Convert the raw words into integers through a word index value\n",
    "        \n",
    "        # Grabs the word index for every word in story\n",
    "        # [9, 34, ...]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grabs the word index for every word in query\n",
    "        q = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Index 0 is reserved since we are using pad sequences, so we add + 1\n",
    "        a = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # a is an empty matrix of NP zeros so we'll use numpy logic to create this assignment (Yes/No)\n",
    "        a[word_index[answer]] = 1\n",
    "        \n",
    "        # We now append each set to their appropriate output list.\n",
    "        X.append(x)\n",
    "        Q.append(q)\n",
    "        A.append(a)\n",
    "    \n",
    "    # Now that we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    X_padded_seqs = pad_sequences(X, maxlen = maxStoryLen)\n",
    "    Q_padded_seqs = pad_sequences(Q, maxlen= maxQueryLen)\n",
    "    answers = np.array(A)\n",
    "    \n",
    "    # Now that the sequences are padded based on their max length, the RNN can be trained on uniformly long sequences.\n",
    "    # Returns tuple for unpacking. \n",
    "    return (X_padded_seqs, Q_padded_seqs, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsTrain, queriesTrain, answersTrain = vectorizeStories(train_data)\n",
    "inputsTest, queriesTest, answersTest = vectorizeStories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 26 19 12]\n",
      " [ 0  0  0 ... 26  1 12]\n",
      " [ 0  0  0 ... 26 23 12]\n",
      " ...\n",
      " [ 0  0  0 ... 26 19 12]\n",
      " [ 0  0  0 ... 30 32 12]\n",
      " [ 0  0  0 ... 21 32 12]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 26 19 12]\n",
      " [ 0  0  0 ... 26 28 12]\n",
      " [ 0  0  0 ... 26 28 12]\n",
      " ...\n",
      " [ 0  0  0 ... 26 21 12]\n",
      " [ 0  0  0 ... 26 28 12]\n",
      " [ 0  0  0 ... 21 32 12]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(answersTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 497.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       503.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answersTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 497 'yes's at index location 7 and 29 'no's at index location 29.\n",
    "\n",
    "Our stories, queries, and answers are now successfully vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 2 inputs: The stories and questions\n",
    "# We'll need to use place holders so we will use `Input()` to insantiate a Keras tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape = (Longest Story, Batch Size)\n",
    "inputSequence = Input((longestStory, ))\n",
    "query = Input((longestQuery, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three encoders we will build\n",
    "* Input Encoder C\n",
    "* Input Encoder M\n",
    "* Question Encoder\n",
    "\n",
    "We are following this model from the paper:\n",
    "![PaperModel](..\\PaperModel.png)\n",
    "*Figure 1: (a): A single layer version of the model. (b): A three layer version of the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderC = Sequential()\n",
    "# Add 2 layers to it\n",
    "inputEncoderC.add(Embedding(input_dim = vocab_size, output_dim = longestQuery))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderC.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderM = Sequential()\n",
    "# Add 2 layers to it\n",
    "# The dimension is set to 64 as the researchers found it to give good results for that vocab size. \n",
    "inputEncoderM.add(Embedding(input_dim = vocab_size, output_dim = 64)) \n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderM.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, embedding dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors. The paper states: The query q is also embedded (again, in the simplest case via another embedding matrix\n",
    "# B with the same dimensions as A) to obtain an internal state u. So the output dimension will match our encoder m.\n",
    "questionEncoder = Sequential()\n",
    "# Add 2 layers to it\n",
    "questionEncoder.add(Embedding(input_dim = vocab_size, output_dim = 64, input_length = longestQuery))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "questionEncoder.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, longestQuery, embedding dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enocoded <-- Encoder(input)\n",
    "# encode the input sequence and questions (which are indices) to sequences of dense vectors\n",
    "# We already have our placeholders for the inputs (inputSequence & query)\n",
    "inputEncodedM = inputEncoderM(inputSequence)\n",
    "inputEncodedC = inputEncoderC(inputSequence)\n",
    "questionEncoded = questionEncoder(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated in the paper: \n",
    "# In the embedding space, we compute the match between u (1st input vector seq.) and each memory m_i (the query) by taking the innner product\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([inputEncodedM, questionEncoded], axes = (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call an activation function on this match (Softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{i}=\\operatorname{Softmax}\\left(u^{T} m_{i}\\right)$$\n",
    "Where\n",
    "$$\\operatorname{Softmax}\\left(z_{i}\\right)=e^{z_{i}} / \\sum_{j} e^{z_{j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this match matrix with the second input vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = add([match, inputEncodedC]) # (samples, longestStory, longestQuery)\n",
    "response_vector = Permute((2, 1))(response_vector)  # (samples, longestQuery, longestStory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_4/concat:0' shape=(?, 6, 220) dtype=float32>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response_vector, questionEncoded])\n",
    "answer # (batch size, 6 x 220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our answer, we are going to reduce it with a RNN, specifically a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce with LSTM\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one more series of Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "# Dense output layer for the vocab size (samples, vocab_size) # YES/NO 0000\n",
    "answer = Dense(vocab_size)(answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a probability distribution over the vocabulary bc we'll essentially see a bunch of zeros except some probability on YES and some probability on NO. we'll pass this into a Softmax in order to turn it into a 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# Building the final model\n",
    "# This answer links together all the encoders (encoder C, encoder M, Question encoder).\n",
    "# This is how we link our model to those encodings.\n",
    "model = Model([inputSequence, query], answer) \n",
    "\n",
    "# We expect to see only high probabilities on YES or NO, but we're not working with a Binary Cross-Entropy Loss \n",
    "# since we we have a larger vocab size than that. Altough we should only expect to se only high probabilities on YES or NO\n",
    "# categorical_crossentropy since we are doing this across the entire vocabulary. \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       multiple             2432        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 6, 64)        2432        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 156, 6)       0           sequential_8[1][0]               \n",
      "                                                                 sequential_9[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 156, 6)       0           dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       multiple             228         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 156, 6)       0           activation_2[0][0]               \n",
      "                                                                 sequential_7[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 6, 156)       0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6, 220)       0           permute_3[0][0]                  \n",
      "                                                                 sequential_9[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 32)           32384       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32)           0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 38)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 38)           1482        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 38)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 38)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 38)           1482        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 38)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 38)           0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 38)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 38)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 38)           0           activation_7[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 41,694\n",
      "Trainable params: 41,694\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
