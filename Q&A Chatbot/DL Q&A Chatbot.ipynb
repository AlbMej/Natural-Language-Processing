{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Question & Answer Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing a chat bot that can answer questions given a set of sentences. The chatbot will use a subset of the Babi Data Set from Facebook Research; it already contains stories(sentences), queries(questions), and answers. \n",
    "Here is a link to the Babi Data Sets and the research paper this is based on:\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "  \n",
    "The bot currently returns a yes or a no to each question asked. However, I plan on integrating a Natural Language Generation component to introduce some meaningful dialogue as well as a speech to text component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating the model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10:1 ratio for training data vs testing data; there are 10,000 points for train_data and 1,000 points for test_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sandra',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'office',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'office', '?'],\n",
       " 'yes')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_sentence = ' '.join(train_data[10][0]) #Story/Sentence\n",
    "query_question = ' '.join(train_data[10][1]) #Query/Question\n",
    "answer = train_data[10][2] #Answer to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Sandra went back to the hallway . Sandra moved to the office .\n",
      "Question:  Is Sandra in the office ?\n",
      "Answer:    yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: \", story_sentence)\n",
    "print(\"Question: \", query_question)\n",
    "print(\"Answer:   \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vocabulary of all of the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set that contains the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + test_data \n",
    "\n",
    "for story, question , answer in all_data:\n",
    "    # Creates a vocabulary of all the distinct words inside our dataset \n",
    "    vocab = vocab | set(story) # vocab ∪ Story. Continuously adds unique words\n",
    "    vocab = vocab | set(question) # vocab ∪ question. Continuously adds unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the two possible answers \n",
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for Keras pad_sequences \n",
    "vocab_size = len(vocab) + 1 # + 1 to add an extra space for a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest story\n",
    "longestStory = max(( (len(data[0])) for data in all_data )) \n",
    "longestQuery = max(( (len(data[1])) for data in all_data )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(longestStory)\n",
    "print(longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates integer encoding for the sequences of words\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(vocab) # This method creates the vocabulary index based on word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?': 1,\n",
       " 'to': 2,\n",
       " 'picked': 3,\n",
       " 'discarded': 4,\n",
       " 'office': 5,\n",
       " 'daniel': 6,\n",
       " 'mary': 7,\n",
       " 'bathroom': 8,\n",
       " 'kitchen': 9,\n",
       " 'put': 10,\n",
       " 'left': 11,\n",
       " 'apple': 12,\n",
       " 'grabbed': 13,\n",
       " 'sandra': 14,\n",
       " 'went': 15,\n",
       " 'no': 16,\n",
       " 'back': 17,\n",
       " 'journeyed': 18,\n",
       " '.': 19,\n",
       " 'in': 20,\n",
       " 'hallway': 21,\n",
       " 'dropped': 22,\n",
       " 'football': 23,\n",
       " 'john': 24,\n",
       " 'milk': 25,\n",
       " 'garden': 26,\n",
       " 'got': 27,\n",
       " 'bedroom': 28,\n",
       " 'down': 29,\n",
       " 'yes': 30,\n",
       " 'took': 31,\n",
       " 'travelled': 32,\n",
       " 'is': 33,\n",
       " 'the': 34,\n",
       " 'moved': 35,\n",
       " 'up': 36,\n",
       " 'there': 37}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStoryText = []\n",
    "trainQueryText = []\n",
    "trainAnswers = []\n",
    "\n",
    "for story, query, answer in train_data:\n",
    "    trainStoryText.append(story)\n",
    "    trainQueryText.append(query)\n",
    "    trainAnswers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Transforms each word in the sentences to a sequence of integers.\n",
    "trainStorySeq = tokenizer.texts_to_sequences(trainStoryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeStories(data, word_index = tokenizer.word_index, maxStoryLen = longestStory, maxQueryLen = longestQuery):\n",
    "    \"\"\"\n",
    "    Vectorizes stories, queries, & answers into padded sequences. \n",
    "   \n",
    "    Parameters: \n",
    "        data: All the data (Stories, Queries, Answers)\n",
    "        word_index: A word index dictionary. Defaulted to our tokenizer.word_index\n",
    "                    Can be overrided to other datasets or other sets of questions\n",
    "        maxStoryLen: Length of the longest story (Will be used for the pad_sequences function)\n",
    "        maxQueryLen: Length of the longest query (Will be used for the pad_sequences function)\n",
    "        \n",
    "        We need the max story & query length because we are using padded sequences; not every story/query \n",
    "        is the same length and our RNN that we're using for training needs everything to be the same length \n",
    "        We'll pad the inputs with 0s in case there's a story or query that is too short. Or we can cut down \n",
    "        a story or query if it is too long.\n",
    "        \n",
    "    Returns: \n",
    "        this (tuple): A tuple of the form (X, Q, A) (padded based on max lengths)    \n",
    "    \"\"\"\n",
    "    X = [] # X := Stories\n",
    "    Q = [] # Q := Queries\n",
    "    A = [] # A := Answers (yes/no)\n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        # Convert the raw words into integers through a word index value\n",
    "        \n",
    "        # Grabs the word index for every word in story\n",
    "        # [9, 34, ...]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grabs the word index for every word in query\n",
    "        q = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Index 0 is reserved since we are using pad sequences, so we add + 1\n",
    "        a = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # a is an empty matrix of NP zeros so we'll use numpy logic to create this assignment (Yes/No)\n",
    "        a[word_index[answer]] = 1\n",
    "        \n",
    "        # We now append each set to their appropriate output list.\n",
    "        X.append(x)\n",
    "        Q.append(q)\n",
    "        A.append(a)\n",
    "    \n",
    "    # Now that we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    X_padded_seqs = pad_sequences(X, maxlen = maxStoryLen)\n",
    "    Q_padded_seqs = pad_sequences(Q, maxlen= maxQueryLen)\n",
    "    answers = np.array(A)\n",
    "    \n",
    "    # Now that the sequences are padded based on their max length, the RNN can be trained on uniformly long sequences.\n",
    "    # Returns tuple for unpacking. \n",
    "    return (X_padded_seqs, Q_padded_seqs, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsTrain, queriesTrain, answersTrain = vectorizeStories(train_data)\n",
    "inputsTest, queriesTest, answersTest = vectorizeStories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 34 28 19]\n",
      " [ 0  0  0 ... 34 21 19]\n",
      " [ 0  0  0 ... 34  8 19]\n",
      " ...\n",
      " [ 0  0  0 ... 34 28 19]\n",
      " [ 0  0  0 ... 25 37 19]\n",
      " [ 0  0  0 ... 12 37 19]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 34 28 19]\n",
      " [ 0  0  0 ... 34 26 19]\n",
      " [ 0  0  0 ... 34 26 19]\n",
      " ...\n",
      " [ 0  0  0 ... 34 12 19]\n",
      " [ 0  0  0 ... 34 26 19]\n",
      " [ 0  0  0 ... 12 37 19]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(answersTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 503.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 497.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answersTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 497 'yes's at index location 7 and 29 'no's at index location 29.\n",
    "\n",
    "Our stories, queries, and answers are now successfully vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 2 inputs: The stories and questions\n",
    "# We'll need to use place holders so we will use `Input()` to insantiate a Keras tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape = (Longest Story, Batch Size)\n",
    "inputSequence = Input((longestStory, ))\n",
    "query = Input((longestQuery, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three encoders we will build\n",
    "* Input Encoder C\n",
    "* Input Encoder M\n",
    "* Question Encoder\n",
    "\n",
    "We are following this model from the paper:\n",
    "![PaperModel](..\\PaperModel.png)\n",
    "*Figure 1: (a): A single layer version of the model. (b): A three layer version of the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderC = Sequential()\n",
    "# Add 2 layers to it\n",
    "inputEncoderC.add(Embedding(input_dim = vocab_size, output_dim = longestQuery))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderC.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderM = Sequential()\n",
    "# Add 2 layers to it\n",
    "inputEncoderM.add(Embedding(input_dim = vocab_size, output_dim = 64))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderM.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, embedding dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors. The paper states: The query q is also embedded (again, in the simplest case via another embedding matrix\n",
    "# B with the same dimensions as A) to obtain an internal state u. So the output dimension will match our encoder m.\n",
    "questionEncoder = Sequential()\n",
    "# Add 2 layers to it\n",
    "questionEncoder.add(Embedding(input_dim = vocab_size, output_dim = 64, input_length = longestStory))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "questionEncoder.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, longestQuery, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
