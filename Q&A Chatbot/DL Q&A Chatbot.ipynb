{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Question & Answer Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be implementing a chat bot that can answer questions given a set of sentences. The chatbot will use a subset of the Babi Data Set from Facebook Research; it already contains stories(sentences), queries(questions), and answers. \n",
    "Here is a link to the Babi Data Sets as well as the paper describing the bAbI tasks in detail:\n",
    "\n",
    "Full Details: https://research.fb.com/downloads/babi/\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "\n",
    "Here is a link to paper this implementation is based on:\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  https://arxiv.org/pdf/1503.08895.pdf\n",
    "  \n",
    "The bot currently returns a yes or a no to each question asked. However, I plan on integrating a Natural Language Generation component to introduce some meaningful dialogue as well as a speech to text component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating the model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/miniconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:232: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp) # List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10:1 ratio for training data vs testing data; there are 10,000 points for train_data and 1,000 points for test_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sandra',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'office',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'office', '?'],\n",
       " 'yes')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_sentence = ' '.join(train_data[10][0]) #Story/Sentence\n",
    "query_question = ' '.join(train_data[10][1]) #Query/Question\n",
    "answer = train_data[10][2] #Answer to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Sandra went back to the hallway . Sandra moved to the office .\n",
      "Question:  Is Sandra in the office ?\n",
      "Answer:    yes\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: \", story_sentence)\n",
    "print(\"Question: \", query_question)\n",
    "print(\"Answer:   \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vocabulary of all of the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set that contains the vocab words\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data + test_data \n",
    "\n",
    "for story, question , answer in all_data:\n",
    "    # Creates a vocabulary of all the distinct words inside our dataset \n",
    "    vocab = vocab | set(story) # vocab ∪ Story. Continuously adds unique words\n",
    "    vocab = vocab | set(question) # vocab ∪ question. Continuously adds unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the two possible answers \n",
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 0 for Keras pad_sequences \n",
    "vocab_size = len(vocab) + 1 # + 1 to add an extra space for a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest story\n",
    "longestStory = max(( (len(data[0])) for data in all_data )) \n",
    "longestQuery = max(( (len(data[1])) for data in all_data )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(longestStory)\n",
    "print(longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates integer encoding for the sequences of words\n",
    "tokenizer = Tokenizer(filters = [])\n",
    "tokenizer.fit_on_texts(vocab) # This method creates the vocabulary index based on word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?': 1,\n",
       " 'sandra': 2,\n",
       " 'the': 3,\n",
       " 'back': 4,\n",
       " 'picked': 5,\n",
       " 'grabbed': 6,\n",
       " 'hallway': 7,\n",
       " 'office': 8,\n",
       " 'garden': 9,\n",
       " 'in': 10,\n",
       " 'apple': 11,\n",
       " 'bathroom': 12,\n",
       " 'put': 13,\n",
       " 'down': 14,\n",
       " 'went': 15,\n",
       " 'john': 16,\n",
       " 'got': 17,\n",
       " 'there': 18,\n",
       " 'discarded': 19,\n",
       " 'bedroom': 20,\n",
       " 'to': 21,\n",
       " 'moved': 22,\n",
       " 'travelled': 23,\n",
       " 'mary': 24,\n",
       " 'milk': 25,\n",
       " 'dropped': 26,\n",
       " 'no': 27,\n",
       " 'took': 28,\n",
       " 'left': 29,\n",
       " 'up': 30,\n",
       " 'football': 31,\n",
       " 'yes': 32,\n",
       " 'kitchen': 33,\n",
       " 'journeyed': 34,\n",
       " '.': 35,\n",
       " 'is': 36,\n",
       " 'daniel': 37}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStoryText = []\n",
    "trainQueryText = []\n",
    "trainAnswers = []\n",
    "\n",
    "for story, query, answer in train_data:\n",
    "    trainStoryText.append(story)\n",
    "    trainQueryText.append(query)\n",
    "    trainAnswers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Transforms each word in the sentences to a sequence of integers.\n",
    "trainStorySeq = tokenizer.texts_to_sequences(trainStoryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeStories(data, word_index = tokenizer.word_index, maxStoryLen = longestStory, maxQueryLen = longestQuery):\n",
    "    \"\"\"\n",
    "    Vectorizes stories, queries, & answers into padded sequences. \n",
    "   \n",
    "    Parameters: \n",
    "        data: All the data (Stories, Queries, Answers)\n",
    "        word_index: A word index dictionary. Defaulted to our tokenizer.word_index\n",
    "                    Can be overrided to other datasets or other sets of questions\n",
    "        maxStoryLen: Length of the longest story (Will be used for the pad_sequences function)\n",
    "        maxQueryLen: Length of the longest query (Will be used for the pad_sequences function)\n",
    "        \n",
    "        We need the max story & query length because we are using padded sequences; not every story/query \n",
    "        is the same length and our RNN that we're using for training needs everything to be the same length \n",
    "        We'll pad the inputs with 0s in case there's a story or query that is too short. Or we can cut down \n",
    "        a story or query if it is too long.\n",
    "        \n",
    "    Returns: \n",
    "        this (tuple): A tuple of the form (X, Q, A) (padded based on max lengths)    \n",
    "    \"\"\"\n",
    "    X = [] # X := Stories\n",
    "    Q = [] # Q := Queries\n",
    "    A = [] # A := Answers (yes/no)\n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        # Convert the raw words into integers through a word index value\n",
    "        \n",
    "        # Grabs the word index for every word in story\n",
    "        # [9, 34, ...]\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grabs the word index for every word in query\n",
    "        q = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Index 0 is reserved since we are using pad sequences, so we add + 1\n",
    "        a = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # a is an empty matrix of NP zeros so we'll use numpy logic to create this assignment (Yes/No)\n",
    "        a[word_index[answer]] = 1\n",
    "        \n",
    "        # We now append each set to their appropriate output list.\n",
    "        X.append(x)\n",
    "        Q.append(q)\n",
    "        A.append(a)\n",
    "    \n",
    "    # Now that we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    X_padded_seqs = pad_sequences(X, maxlen = maxStoryLen)\n",
    "    Q_padded_seqs = pad_sequences(Q, maxlen= maxQueryLen)\n",
    "    answers = np.array(A)\n",
    "    \n",
    "    # Now that the sequences are padded based on their max length, the RNN can be trained on uniformly long sequences.\n",
    "    # Returns tuple for unpacking. \n",
    "    return (X_padded_seqs, Q_padded_seqs, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsTrain, queriesTrain, answersTrain = vectorizeStories(train_data)\n",
    "inputsTest, queriesTest, answersTest = vectorizeStories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  3 20 35]\n",
      " [ 0  0  0 ...  3  7 35]\n",
      " [ 0  0  0 ...  3 12 35]\n",
      " ...\n",
      " [ 0  0  0 ...  3 20 35]\n",
      " [ 0  0  0 ... 25 18 35]\n",
      " [ 0  0  0 ... 11 18 35]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  3 20 35]\n",
      " [ 0  0  0 ...  3  9 35]\n",
      " [ 0  0  0 ...  3  9 35]\n",
      " ...\n",
      " [ 0  0  0 ...  3 11 35]\n",
      " [ 0  0  0 ...  3  9 35]\n",
      " [ 0  0  0 ... 11 18 35]]\n"
     ]
    }
   ],
   "source": [
    "print(inputsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(answersTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 503.,   0.,   0.,   0.,   0., 497.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(answersTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 497 'yes's at index location 7 and 29 'no's at index location 29.\n",
    "\n",
    "Our stories, queries, and answers are now successfully vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 2 inputs: The stories and questions\n",
    "# We'll need to use place holders so we will use `Input()` to insantiate a Keras tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape = (Longest Story, Batch Size)\n",
    "inputSequence = Input((longestStory, ))\n",
    "query = Input((longestQuery, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three encoders we will build\n",
    "* Input Encoder C\n",
    "* Input Encoder M\n",
    "* Question Encoder\n",
    "\n",
    "We are following this model from the paper:\n",
    "![PaperModel](.\\PaperModel.png)\n",
    "<i><center>Figure 1: (a): A single layer version of the model. (b): A three layer version of the model.</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderC = Sequential()\n",
    "# Add 2 layers to it\n",
    "inputEncoderC.add(Embedding(input_dim = vocab_size, output_dim = longestQuery))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderC.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, longestQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Encoder M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors\n",
    "inputEncoderM = Sequential()\n",
    "# Add 2 layers to it\n",
    "# The dimension is set to 64 as the researchers found it to give good results for that vocab size. \n",
    "inputEncoderM.add(Embedding(input_dim = vocab_size, output_dim = 64)) \n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "inputEncoderM.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, stories max len, embedding dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This input gets embedding to a sequence of vectors. The paper states: The query q is also embedded (again, in the simplest case via another embedding matrix\n",
    "# B with the same dimensions as A) to obtain an internal state u. So the output dimension will match our encoder m.\n",
    "questionEncoder = Sequential()\n",
    "# Add 2 layers to it\n",
    "questionEncoder.add(Embedding(input_dim = vocab_size, output_dim = 64, input_length = longestQuery))\n",
    "# Turns off a random % of nuerons. Helps with overfitting. Can increase droupout and train longer if wanted\n",
    "questionEncoder.add(Dropout(0.3))\n",
    "\n",
    "# This encoder will output:\n",
    "# output: (samples, longestQuery, embedding dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enocoded <-- Encoder(input)\n",
    "# encode the input sequence and questions (which are indices) to sequences of dense vectors\n",
    "# We already have our placeholders for the inputs (inputSequence & query)\n",
    "inputEncodedM = inputEncoderM(inputSequence)\n",
    "inputEncodedC = inputEncoderC(inputSequence)\n",
    "questionEncoded = questionEncoder(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated in the paper: \n",
    "# In the embedding space, we compute the match between u (1st input vector seq.) and each memory m_i (the query) by taking the innner product\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([inputEncodedM, questionEncoded], axes = (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call an activation function on this match (Softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{i}=\\operatorname{Softmax}\\left(u^{T} m_{i}\\right)$$\n",
    "Where\n",
    "$$\\operatorname{Softmax}\\left(z_{i}\\right)=e^{z_{i}} / \\sum_{j} e^{z_{j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this match matrix with the second input vector sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = add([match, inputEncodedC]) # (samples, longestStory, longestQuery)\n",
    "response_vector = Permute((2, 1))(response_vector)  # (samples, longestQuery, longestStory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_4/concat:0' shape=(?, 6, 220) dtype=float32>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response_vector, questionEncoded])\n",
    "answer # (batch size, 6 x 220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our answer, we are going to reduce it with a RNN, specifically a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce with LSTM\n",
    "answer = LSTM(32)(answer)  # (samples, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one more series of Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "# Dense output layer for the vocab size (samples, vocab_size) # YES/NO 0000\n",
    "answer = Dense(vocab_size)(answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a probability distribution over the vocabulary bc we'll essentially see a bunch of zeros except some probability on YES and some probability on NO. we'll pass this into a Softmax in order to turn it into a 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# Building the final model\n",
    "# This answer links together all the encoders (encoder C, encoder M, Question encoder).\n",
    "# This is how we link our model to those encodings.\n",
    "model = Model([inputSequence, query], answer) \n",
    "\n",
    "# We expect to see only high probabilities on YES or NO, but we're not working with a Binary Cross-Entropy Loss \n",
    "# since we we have a larger vocab size than that. Altough we should only expect to see only high probabilities on YES or NO\n",
    "# categorical_crossentropy since we are doing this across the entire vocabulary. \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       multiple             2432        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 6, 64)        2432        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 156, 6)       0           sequential_5[1][0]               \n",
      "                                                                 sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 156, 6)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       multiple             228         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 156, 6)       0           activation_3[0][0]               \n",
      "                                                                 sequential_4[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 6, 156)       0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6, 220)       0           permute_3[0][0]                  \n",
      "                                                                 sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 32)           32384       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32)           0           lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 38)           1254        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 463us/step - loss: 0.5937 - acc: 0.6945 - val_loss: 0.5673 - val_acc: 0.7170\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 4s 395us/step - loss: 0.5539 - acc: 0.7317 - val_loss: 0.5375 - val_acc: 0.7530\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 6s 562us/step - loss: 0.5232 - acc: 0.7538 - val_loss: 0.4943 - val_acc: 0.7780\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.5005 - acc: 0.7708 - val_loss: 0.4650 - val_acc: 0.7870\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.4803 - acc: 0.7824 - val_loss: 0.4459 - val_acc: 0.7930\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.4635 - acc: 0.7864 - val_loss: 0.4385 - val_acc: 0.7940\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 4s 385us/step - loss: 0.4462 - acc: 0.8018 - val_loss: 0.4348 - val_acc: 0.8000\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.4251 - acc: 0.8145 - val_loss: 0.4338 - val_acc: 0.8080\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 4s 379us/step - loss: 0.4082 - acc: 0.8242 - val_loss: 0.4299 - val_acc: 0.8140\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.4020 - acc: 0.8298 - val_loss: 0.4080 - val_acc: 0.8210\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 4s 406us/step - loss: 0.3912 - acc: 0.8322 - val_loss: 0.3954 - val_acc: 0.8270\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 4s 426us/step - loss: 0.3824 - acc: 0.8390 - val_loss: 0.4057 - val_acc: 0.8290\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 4s 400us/step - loss: 0.3703 - acc: 0.8478 - val_loss: 0.3863 - val_acc: 0.8240\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.3631 - acc: 0.8476 - val_loss: 0.3877 - val_acc: 0.8300\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.3615 - acc: 0.8455 - val_loss: 0.3820 - val_acc: 0.8270\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3478 - acc: 0.8539 - val_loss: 0.3792 - val_acc: 0.8170\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.3473 - acc: 0.8524 - val_loss: 0.3849 - val_acc: 0.8280\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3380 - acc: 0.8581 - val_loss: 0.3826 - val_acc: 0.8310\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3354 - acc: 0.8542 - val_loss: 0.3679 - val_acc: 0.8290\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 4s 432us/step - loss: 0.3336 - acc: 0.8565 - val_loss: 0.3978 - val_acc: 0.8360\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 4s 419us/step - loss: 0.3289 - acc: 0.8603 - val_loss: 0.3784 - val_acc: 0.8310\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.3270 - acc: 0.8572 - val_loss: 0.3720 - val_acc: 0.8280\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.3238 - acc: 0.8611 - val_loss: 0.3597 - val_acc: 0.8290\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.3247 - acc: 0.8621 - val_loss: 0.3648 - val_acc: 0.8250\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.3148 - acc: 0.8652 - val_loss: 0.3626 - val_acc: 0.8280\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.3139 - acc: 0.8647 - val_loss: 0.3739 - val_acc: 0.8360\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3083 - acc: 0.8689 - val_loss: 0.3843 - val_acc: 0.8390\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.3024 - acc: 0.8726 - val_loss: 0.3711 - val_acc: 0.8150\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.3093 - acc: 0.8665 - val_loss: 0.3577 - val_acc: 0.8250\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.3082 - acc: 0.8679 - val_loss: 0.3580 - val_acc: 0.8220\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.3017 - acc: 0.8695 - val_loss: 0.3531 - val_acc: 0.8310\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.2968 - acc: 0.8718 - val_loss: 0.3540 - val_acc: 0.8230\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.3031 - acc: 0.8719 - val_loss: 0.3606 - val_acc: 0.8270\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.2909 - acc: 0.8744 - val_loss: 0.3887 - val_acc: 0.8280\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.2961 - acc: 0.8715 - val_loss: 0.3668 - val_acc: 0.8260\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 4s 383us/step - loss: 0.2947 - acc: 0.8741 - val_loss: 0.3781 - val_acc: 0.8360\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 4s 380us/step - loss: 0.2885 - acc: 0.8741 - val_loss: 0.3756 - val_acc: 0.8240\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.2902 - acc: 0.8734 - val_loss: 0.3788 - val_acc: 0.8300\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 4s 393us/step - loss: 0.2876 - acc: 0.8732 - val_loss: 0.3750 - val_acc: 0.8250\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 4s 384us/step - loss: 0.2858 - acc: 0.8759 - val_loss: 0.4128 - val_acc: 0.8340\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 4s 384us/step - loss: 0.2780 - acc: 0.8816 - val_loss: 0.3971 - val_acc: 0.8310\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 4s 383us/step - loss: 0.2801 - acc: 0.8785 - val_loss: 0.3850 - val_acc: 0.8130\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 4s 392us/step - loss: 0.2809 - acc: 0.8793 - val_loss: 0.3761 - val_acc: 0.8250\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.2776 - acc: 0.8791 - val_loss: 0.3836 - val_acc: 0.8300\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 4s 389us/step - loss: 0.2705 - acc: 0.8832 - val_loss: 0.4089 - val_acc: 0.8140\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 4s 382us/step - loss: 0.2728 - acc: 0.8796 - val_loss: 0.3785 - val_acc: 0.8220\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 4s 385us/step - loss: 0.2695 - acc: 0.8830 - val_loss: 0.3949 - val_acc: 0.8200\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.2708 - acc: 0.8837 - val_loss: 0.3833 - val_acc: 0.8160\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.2644 - acc: 0.8859 - val_loss: 0.4092 - val_acc: 0.8230\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.2663 - acc: 0.8835 - val_loss: 0.4093 - val_acc: 0.8310\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 4s 394us/step - loss: 0.2627 - acc: 0.8869 - val_loss: 0.4407 - val_acc: 0.8340\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 4s 388us/step - loss: 0.2652 - acc: 0.8836 - val_loss: 0.4170 - val_acc: 0.8150\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 4s 402us/step - loss: 0.2584 - acc: 0.8878 - val_loss: 0.4298 - val_acc: 0.8290\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.2573 - acc: 0.8874 - val_loss: 0.4153 - val_acc: 0.8230\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 4s 391us/step - loss: 0.2601 - acc: 0.8872 - val_loss: 0.4068 - val_acc: 0.8220\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 4s 394us/step - loss: 0.2566 - acc: 0.8906 - val_loss: 0.4194 - val_acc: 0.8220\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 4s 429us/step - loss: 0.2525 - acc: 0.8892 - val_loss: 0.4530 - val_acc: 0.8080\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 4s 423us/step - loss: 0.2486 - acc: 0.8907 - val_loss: 0.4461 - val_acc: 0.8220\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2485 - acc: 0.8923 - val_loss: 0.4509 - val_acc: 0.8120\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 4s 371us/step - loss: 0.2418 - acc: 0.8928 - val_loss: 0.4556 - val_acc: 0.8230\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 4s 381us/step - loss: 0.2475 - acc: 0.8928 - val_loss: 0.4536 - val_acc: 0.8120\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.2461 - acc: 0.8953 - val_loss: 0.4708 - val_acc: 0.8090\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.2431 - acc: 0.8964 - val_loss: 0.4651 - val_acc: 0.8150\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2405 - acc: 0.8978 - val_loss: 0.4909 - val_acc: 0.8210\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2414 - acc: 0.8971 - val_loss: 0.4680 - val_acc: 0.8140\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 4s 373us/step - loss: 0.2326 - acc: 0.8997 - val_loss: 0.5245 - val_acc: 0.8060\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2327 - acc: 0.8991 - val_loss: 0.5244 - val_acc: 0.8030\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2327 - acc: 0.8963 - val_loss: 0.5369 - val_acc: 0.8030\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2273 - acc: 0.8990 - val_loss: 0.4944 - val_acc: 0.8170\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2285 - acc: 0.9022 - val_loss: 0.4878 - val_acc: 0.8190\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.2317 - acc: 0.9005 - val_loss: 0.5025 - val_acc: 0.8140\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.2298 - acc: 0.9032 - val_loss: 0.5040 - val_acc: 0.8130\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.2273 - acc: 0.9063 - val_loss: 0.5124 - val_acc: 0.8110\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 4s 375us/step - loss: 0.2263 - acc: 0.9014 - val_loss: 0.5126 - val_acc: 0.8120\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.2266 - acc: 0.9023 - val_loss: 0.5108 - val_acc: 0.8140\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2200 - acc: 0.9035 - val_loss: 0.5335 - val_acc: 0.8240\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2149 - acc: 0.9100 - val_loss: 0.5281 - val_acc: 0.8150\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 4s 400us/step - loss: 0.2181 - acc: 0.9063 - val_loss: 0.5912 - val_acc: 0.8120\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 4s 401us/step - loss: 0.2173 - acc: 0.9077 - val_loss: 0.5939 - val_acc: 0.8130\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 4s 386us/step - loss: 0.2143 - acc: 0.9087 - val_loss: 0.5576 - val_acc: 0.8140\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 4s 378us/step - loss: 0.2148 - acc: 0.9089 - val_loss: 0.5671 - val_acc: 0.8120\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 4s 376us/step - loss: 0.2120 - acc: 0.9093 - val_loss: 0.5674 - val_acc: 0.8110\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 4s 377us/step - loss: 0.2130 - acc: 0.9094 - val_loss: 0.6157 - val_acc: 0.8050\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 4s 389us/step - loss: 0.2109 - acc: 0.9101 - val_loss: 0.5736 - val_acc: 0.8000\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 5s 478us/step - loss: 0.2090 - acc: 0.9115 - val_loss: 0.5832 - val_acc: 0.8170\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 5s 534us/step - loss: 0.2109 - acc: 0.9087 - val_loss: 0.5965 - val_acc: 0.8160\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.2065 - acc: 0.9144 - val_loss: 0.6670 - val_acc: 0.8120\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 6s 630us/step - loss: 0.2055 - acc: 0.9126 - val_loss: 0.5659 - val_acc: 0.8070\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 4s 425us/step - loss: 0.2055 - acc: 0.9111 - val_loss: 0.6222 - val_acc: 0.8160\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 4s 449us/step - loss: 0.2093 - acc: 0.9102 - val_loss: 0.6229 - val_acc: 0.8150\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 4s 422us/step - loss: 0.2036 - acc: 0.9142 - val_loss: 0.6551 - val_acc: 0.8140\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.1999 - acc: 0.9133 - val_loss: 0.5758 - val_acc: 0.8130\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 4s 399us/step - loss: 0.2029 - acc: 0.9149 - val_loss: 0.6036 - val_acc: 0.8120\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 4s 398us/step - loss: 0.2003 - acc: 0.9171 - val_loss: 0.6702 - val_acc: 0.8090\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 4s 396us/step - loss: 0.2005 - acc: 0.9145 - val_loss: 0.6523 - val_acc: 0.8060\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 4s 418us/step - loss: 0.1994 - acc: 0.9167 - val_loss: 0.6529 - val_acc: 0.8140\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 4s 424us/step - loss: 0.1957 - acc: 0.9179 - val_loss: 0.6226 - val_acc: 0.8120\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 4s 419us/step - loss: 0.1983 - acc: 0.9171 - val_loss: 0.6186 - val_acc: 0.8130\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 4s 425us/step - loss: 0.1882 - acc: 0.9176 - val_loss: 0.6556 - val_acc: 0.8110\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 4s 434us/step - loss: 0.1989 - acc: 0.9161 - val_loss: 0.6758 - val_acc: 0.8060\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 4s 408us/step - loss: 0.1898 - acc: 0.9173 - val_loss: 0.6764 - val_acc: 0.8070\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 4s 404us/step - loss: 0.1966 - acc: 0.9169 - val_loss: 0.6452 - val_acc: 0.8020\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 4s 408us/step - loss: 0.1932 - acc: 0.9203 - val_loss: 0.6715 - val_acc: 0.8150\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 4s 408us/step - loss: 0.1915 - acc: 0.9207 - val_loss: 0.6581 - val_acc: 0.8050\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 4s 416us/step - loss: 0.1855 - acc: 0.9218 - val_loss: 0.6700 - val_acc: 0.8120\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 4s 408us/step - loss: 0.1875 - acc: 0.9208 - val_loss: 0.6821 - val_acc: 0.8070\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.1833 - acc: 0.9240 - val_loss: 0.6825 - val_acc: 0.8070\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 4s 416us/step - loss: 0.1825 - acc: 0.9235 - val_loss: 0.6886 - val_acc: 0.8050\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 4s 414us/step - loss: 0.1868 - acc: 0.9233 - val_loss: 0.7190 - val_acc: 0.8070\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 4s 413us/step - loss: 0.1829 - acc: 0.9243 - val_loss: 0.6874 - val_acc: 0.8100\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 4s 415us/step - loss: 0.1804 - acc: 0.9232 - val_loss: 0.6885 - val_acc: 0.8110\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 4s 434us/step - loss: 0.1888 - acc: 0.9244 - val_loss: 0.6618 - val_acc: 0.8140\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 4s 412us/step - loss: 0.1878 - acc: 0.9215 - val_loss: 0.7190 - val_acc: 0.8030\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 4s 417us/step - loss: 0.1871 - acc: 0.9223 - val_loss: 0.6986 - val_acc: 0.8140\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 4s 418us/step - loss: 0.1787 - acc: 0.9220 - val_loss: 0.7510 - val_acc: 0.8040\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 4s 418us/step - loss: 0.1859 - acc: 0.9255 - val_loss: 0.6963 - val_acc: 0.8020\n",
      "Epoch 117/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 403us/step - loss: 0.1823 - acc: 0.9242 - val_loss: 0.6967 - val_acc: 0.8050\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 4s 411us/step - loss: 0.1795 - acc: 0.9254 - val_loss: 0.6690 - val_acc: 0.8070\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 4s 389us/step - loss: 0.1739 - acc: 0.9296 - val_loss: 0.7425 - val_acc: 0.8100\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 4s 406us/step - loss: 0.1808 - acc: 0.9268 - val_loss: 0.6730 - val_acc: 0.8210\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the model for 3 epochs, in batches of 16 samples,\n",
    "on data stored in the Numpy array X_train,\n",
    "and labels stored in the Numpy array y_train:\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=16, verbose=1)\n",
    "\n",
    "What you will see with mode verbose=1:\n",
    "Train on 37800 samples, validate on 4200 samples\n",
    "Epoch 0\n",
    "37800/37800 [==============================] - 7s - loss: 0.0385\n",
    "Epoch 1\n",
    "37800/37800 [==============================] - 8s - loss: 0.0140\n",
    "Epoch 2\n",
    "10960/37800 [=======>......................] - ETA: 4s - loss: 0.0109\n",
    "\n",
    "Smaller batch sizes with longer training epochs should lead to slightly better results. \n",
    "'''\n",
    "# Training on input stories and input questions\n",
    "x_train = [inputsTrain, queriesTrain]\n",
    "correctLabels = answersTrain\n",
    "\n",
    "validSet = ([inputsTest, queriesTest], answersTest)\n",
    "\n",
    "chatbotHistory = model.fit(x_train, correctLabels, batch_size=32, epochs=120, validation_data=validSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pprint(chatbotHistory.history)\n",
    "print(chatbotHistory.history.keys())\n",
    "# Summarize the history for accuracy\n",
    "#Loss & and Accuracy are essentially inverses of each other. We are trying to increase accuracy/reduce loss\n",
    "plt.plot(chatbotHistory.history['acc']) \n",
    "plt.plot(chatbotHistory.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/chatbot_120_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a different model\n",
    "# model.load_weights(\"models/chatbot_10_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "# Trying to predict based on the test set so the answer labels are not passed in\n",
    "predictedResults = model.predict((validSet[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: Mary got the milk there . John moved to the bedroom .\n",
      "Question: Is John in the kitchen ?\n",
      "Correct Answer:  no\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[0][0])\n",
    "print(f'Story: {story}')\n",
    "\n",
    "query = ' '.join(word for word in test_data[0][1])\n",
    "print(f'Question: {query}')\n",
    "\n",
    "print(\"Correct Answer: \",test_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 38)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedResults.shape #37 vocab words + 1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9816977e-15, 2.0368580e-15, 2.3015473e-15, 1.9834979e-15,\n",
       "       2.3366147e-15, 2.1451880e-15, 1.8299696e-15, 2.1858632e-15,\n",
       "       2.4458945e-15, 1.8294811e-15, 2.1176095e-15, 2.0684839e-15,\n",
       "       1.9871332e-15, 2.2822343e-15, 2.0055853e-15, 2.1186357e-15,\n",
       "       2.2708581e-15, 2.2851354e-15, 1.8959331e-15, 2.1542661e-15,\n",
       "       2.0863381e-15, 2.0676320e-15, 2.4000691e-15, 2.3236106e-15,\n",
       "       2.2063965e-15, 2.0691865e-15, 1.8461805e-15, 9.9997878e-01,\n",
       "       2.2069016e-15, 2.1036722e-15, 2.2178560e-15, 1.9348174e-15,\n",
       "       2.1226384e-05, 2.5996413e-15, 2.1325740e-15, 1.9930545e-15,\n",
       "       2.1961269e-15, 2.1997153e-15], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedResults[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostyProbableWord = np.argmax(predictedResults[0]) #Word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(mostyProbableWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.9999788\n"
     ]
    }
   ],
   "source": [
    "#Generate the prediction from model\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    #Searching for the word\n",
    "    if val == mostyProbableWord:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", predictedResults[0][mostyProbableWord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very confident at 120 epochs. Although it appears that it wasn't necessary to train for 120 epochs. Probably could've trained for less. Will do an analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To expand training and test set, I need to create new stories, new questions, and correct answers.\n",
    "# Currently limited to using only the words found in the vocabulary the network was trained on. \n",
    "# Probably could make a simple one to showcase it's potential for talking about business related conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will check prediction on my own story and my own ques (limited to using the vocab words of course) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
